2/25

starter_scripts:

    generate_results.py: the place where you build your submission

                        (similiar to TrackFactory class in ppd)

    generate_submission.py: 

        writes to random_submission.json

    
scorer_scripts:

    score_detections.py

        -g 

    python scorer_scripts_v2/scorer_scripts_v2/score_detections.py -g starter_scripts_v2/starter_scripts_v2/training_GT_labels_v2.json -p starter_scripts_v2/starter_scripts_v2/random_submission.json

look for people on github with these file names?
lookn for code keywords?

we don't get labels for test dataset,
    but we could build those our self

iiuc, the leaderboard images are the exact ones we're operating on
for our score on the leaderboard

what is % score on the leaderboard for accuracy only right?

where in the submission.json is CI being written?

IuC, mAP - define

what is the 9th position in random_submission file? why 0.5?
    the ground truth only has eight

resolve the labels / "ground truth" file versioning controversy
    i have v2, it was uploaded when ?
    when is the latest complaint on the forum ?
    Feb20 was complaint/response

do we upload code for test2 or only test3?

why does ground truth only have one bounding box?

no gate -> submission should have empty brackets

we can use scorer to figure out the expected score from a naive/baseline avg algo
    -> avg coord from training_labels.json
        adjust for vert/horiz align
        or we can just hand score em

free gpu instances?
https://www.herox.com/alphapilot/resource/285

most concise description: https://www.herox.com/alphapilot/77-test-2

    The internal clearance is 8ft x 8ft, the external dimensions are 11ft x 11ft, and the depth is 1ft.

    libraries should be compatible with Python 3.5.2

ArUco markers / AR markers

    https://docs.opencv.org/3.4.2/d5/dae/tutorial_aruco_detection.html

    https://pypi.org/project/ar-markers/

    https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/
        import pyzbar

    https://stackoverflow.com/questions/50433482/how-to-detect-a-pattern-such-as-a-qr-code-on-a-scanned-paper-so-that-it-can-be


stuff to get:

    shapely package?
        from wheel in misc/
            Shapely-1.6.4.post1-cp27-cp27m-win32.whl
        downloaded here:
            https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely

    python3 - need time.monotonic()

    vott - img/vid tagging
    https://github.com/Microsoft/VoTT

TODO

    [x] correct glob issue, path name in submission.json keys
    [x] run score_detections on training data

        python scorer_scripts_v2/scorer_scripts_v2/score_detections.py -g training_GT_labels_v2.json -p random_submission3.json

    [x] plot in matplotlib in notebook

    [x] cv draw polygon
    
    [x] build a avg point submission
        [x] get it's score
        [x] first, just with inner box
        [x] run pred=truth, cocoMap? 0.91
        [ ] build mini training set
        [x] build second box, score
        
        inner rect is 8'x8', and frame is 3'
            -> ratio is 3/8 *width or 3/8*height

        second box won't help our local score:
            the ground truth has no second box!

    [~] how does generate_submission build something different than
        our mean_prediction.json?
        -> still, idk

2/26

    [x] build mini training-set

    [x] score() takes dict not fn

    [ ] utils gets calcMean functions
        [ ] run on mini1 vs mini2

    [ ] pred=actual on mini-sets

    [ ] label images task
        [x] run program
        [~] determine my block 7500 - 7999
            [~] verify this with other submissions
        [ ] start a trial
            [ ] verify -j output works
            [ ] verify an alter frame
            [ ] find a mislabel

    [ ] breakout cocoMAP for individual images / indvidual corners

    [ ] perform search on baseline ideal rect

    [ ] build roiSelect tool (for leader set)

    [ ] bring in pyzbar, try to work it




